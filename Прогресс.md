generative-model-without-encoder на основе имеющейся D2NN модели

Были сгенерированы изображения рукописных цифр на базе MNIST. Обучение производилось только по одной цифре (только по нулям из MNIST, только по четверкам из MNIST и т.п.).
Поэтому, предположительно, модель обучилась не генерировать цифру, а вырезать трафарет какой-то определенной цифры. 
гиперпараметры:

#Гиперпараметры
n = 64
pixels = 64
length = 0.001
wavelength = 500E-9
masks = 3
distance = 0.08628599497985633
lr = 0.1
batch_size = 32
optimizer - Adam
pading: 0.5
err_func: MSE

32х32:

![нолик 1](https://github.com/user-attachments/assets/514c34a4-a72b-49f0-a04e-317986164e42)
![четверка](https://github.com/user-attachments/assets/4bd82ab7-2797-428d-8add-439411502cb6)
![восьмерка](https://github.com/user-attachments/assets/f3c842a0-ead7-4b0f-962a-8f058c6cc0f5)

64х64:

![восьмёрка 2](https://github.com/user-attachments/assets/5b93385b-3f5e-49b5-a669-205966483eb1)

Что будет, если обучать по всем цифрам, а не по одной заданной:

![bull shit](https://github.com/user-attachments/assets/10fcea9f-d47e-46cf-b943-14d97b0c82f6)

Похоже на какую-то суперпозицию многих цифр

