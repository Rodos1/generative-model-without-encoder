generative-model-without-encoder на основе имеющейся D2NN модели

Были сгенерированы изображения рукописных цифр на базе MNIST. Обучение производилось только по одной цифре (только по нулям из MNIST, только по четверкам из MNIST и т.п.).
Поэтому, предположительно, модель обучилась не генерировать цифру, а вырезать трафарет какой-то определенной цифры. 
гиперпараметры:

#Гиперпараметры
n = 64
pixels = 64
length = 0.001
wavelength = 500E-9
masks = 3
distance = 0.08628599497985633
lr = 0.1
batch_size = 32
optimizer - Adam
pading: 0.5
err_func: MSE

32х32:

![нолик 1](https://github.com/user-attachments/assets/92a16213-1615-4444-99b1-9afda798dd43)
![пятерка](https://github.com/user-attachments/assets/4694988e-2d17-4c87-a2fd-8c493f72d5c2)
![восьмёрка 2](https://github.com/user-attachments/assets/b2b876b1-d527-4212-9932-7ad009325588)

64х64:

![восьмерка](https://github.com/user-attachments/assets/cea6222e-7bd5-4708-be08-1493bd5cdb4e)

Что будет, если обучать по всем цифрам, а не по одной заданной:

![bull shit](https://github.com/user-attachments/assets/10fcea9f-d47e-46cf-b943-14d97b0c82f6)

Похоже на какую-то суперпозицию многих цифр

